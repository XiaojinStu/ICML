==========================================
ANE v3 Production Experiments
==========================================
Samples: 50
Update Target: attn+ln
Learning Rate: 0.0005
Optimizer: sgd
Output: results_ane

=== Llama-3.2-1B Experiments ===

>>> 1B Model, 5 steps
2026-01-13 00:23:16.912108: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-13 00:23:16.967781: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

======================================================================
ANE-TTA v3 Experiment: llama1b_steps5
======================================================================
Model: Llama-3.2-1B-Instruct
Model size: 1.2B parameters
Numerical tokens: 1110/128000
Model size: 1.2B params
Training 16/16 layers
Update target: attn+ln
Trainable params: 167.8M (13.6%)
Processing samples:   0%|          | 0/50 [00:00<?, ?it/s]Processing samples:   2%|▏         | 1/50 [00:00<00:44,  1.11it/s]Processing samples:   4%|▍         | 2/50 [00:01<00:26,  1.84it/s]Processing samples:   6%|▌         | 3/50 [00:01<00:25,  1.82it/s]Processing samples:   8%|▊         | 4/50 [00:02<00:25,  1.79it/s]Processing samples:  10%|█         | 5/50 [00:02<00:26,  1.70it/s]Processing samples:  12%|█▏        | 6/50 [00:03<00:30,  1.43it/s]Processing samples:  14%|█▍        | 7/50 [00:04<00:32,  1.32it/s]Processing samples:  16%|█▌        | 8/50 [00:05<00:34,  1.23it/s]Processing samples:  18%|█▊        | 9/50 [00:06<00:37,  1.09it/s]Processing samples:  20%|██        | 10/50 [00:08<00:40,  1.02s/it]Processing samples:  22%|██▏       | 11/50 [00:09<00:41,  1.08s/it]Processing samples:  24%|██▍       | 12/50 [00:10<00:42,  1.11s/it]Processing samples:  26%|██▌       | 13/50 [00:12<00:45,  1.24s/it]Processing samples:  28%|██▊       | 14/50 [00:13<00:47,  1.33s/it]Processing samples:  30%|███       | 15/50 [00:14<00:47,  1.36s/it]Processing samples:  32%|███▏      | 16/50 [00:16<00:49,  1.46s/it]Processing samples:  34%|███▍      | 17/50 [00:18<00:52,  1.60s/it]Processing samples:  36%|███▌      | 18/50 [00:20<00:56,  1.76s/it]Processing samples:  38%|███▊      | 19/50 [00:22<00:57,  1.85s/it]Processing samples:  40%|████      | 20/50 [00:25<00:59,  1.97s/it]Processing samples:  42%|████▏     | 21/50 [00:27<01:02,  2.16s/it]Processing samples:  44%|████▍     | 22/50 [00:30<01:04,  2.29s/it]Processing samples:  46%|████▌     | 23/50 [00:32<01:04,  2.39s/it]Processing samples:  48%|████▊     | 24/50 [00:35<01:05,  2.53s/it]Processing samples:  50%|█████     | 25/50 [00:38<01:04,  2.59s/it]Processing samples:  52%|█████▏    | 26/50 [00:41<01:03,  2.66s/it]Processing samples:  54%|█████▍    | 27/50 [00:44<01:04,  2.81s/it]Processing samples:  56%|█████▌    | 28/50 [00:47<01:04,  2.94s/it]Processing samples:  58%|█████▊    | 29/50 [00:50<01:03,  3.03s/it]Processing samples:  60%|██████    | 30/50 [00:54<01:01,  3.09s/it]Processing samples:  62%|██████▏   | 31/50 [00:57<00:58,  3.07s/it]Processing samples:  64%|██████▍   | 32/50 [01:00<00:56,  3.15s/it]Processing samples:  66%|██████▌   | 33/50 [01:04<00:55,  3.28s/it]Processing samples:  68%|██████▊   | 34/50 [01:07<00:54,  3.43s/it]Processing samples:  70%|███████   | 35/50 [01:11<00:52,  3.50s/it]Processing samples:  72%|███████▏  | 36/50 [01:15<00:50,  3.63s/it]Processing samples:  74%|███████▍  | 37/50 [01:19<00:48,  3.70s/it]Processing samples:  76%|███████▌  | 38/50 [01:23<00:45,  3.83s/it]Processing samples:  78%|███████▊  | 39/50 [01:27<00:43,  3.91s/it]Processing samples:  80%|████████  | 40/50 [01:32<00:40,  4.08s/it]Processing samples:  82%|████████▏ | 41/50 [01:36<00:37,  4.11s/it]Processing samples:  84%|████████▍ | 42/50 [01:40<00:33,  4.22s/it]Processing samples:  86%|████████▌ | 43/50 [01:45<00:30,  4.29s/it]Processing samples:  88%|████████▊ | 44/50 [01:49<00:26,  4.44s/it]Processing samples:  90%|█████████ | 45/50 [01:54<00:22,  4.52s/it]Processing samples:  92%|█████████▏| 46/50 [01:59<00:18,  4.70s/it]Processing samples:  94%|█████████▍| 47/50 [02:04<00:14,  4.67s/it]Processing samples:  96%|█████████▌| 48/50 [02:08<00:09,  4.64s/it]Processing samples:  98%|█████████▊| 49/50 [02:13<00:04,  4.74s/it]Processing samples: 100%|██████████| 50/50 [02:19<00:00,  4.95s/it]Processing samples: 100%|██████████| 50/50 [02:19<00:00,  2.79s/it]

======================================================================
Results Summary
======================================================================
Total tokens: 453
Accuracy Before TTA: 0.0287 (13/453)
Accuracy After TTA:  0.0309 (14/453)
Improvement: +0.0022
Flipped cases (wrong->correct): 2
Total time: 2.6 min
Time per token: 0.34 sec
======================================================================
Results saved to: results_ane/llama1b_steps5.json

Generating visualizations...
Generating visualizations for llama1b_steps5...
  Saved: results_ane/llama1b_steps5_prob_heatmap_page1.png
  Saved: results_ane/llama1b_steps5_prob_heatmap_page2.png
  Saved: results_ane/llama1b_steps5_prob_heatmap_page3.png
  Saved: results_ane/llama1b_steps5_prob_heatmap_page4.png
  Saved: results_ane/llama1b_steps5_prob_heatmap_page5.png
  Saved: results_ane/llama1b_steps5_prob_heatmap_page6.png
  Saved: results_ane/llama1b_steps5_rank_heatmap_page1.png
  Saved: results_ane/llama1b_steps5_rank_heatmap_page2.png
  Saved: results_ane/llama1b_steps5_rank_heatmap_page3.png
  Saved: results_ane/llama1b_steps5_rank_heatmap_page4.png
  Saved: results_ane/llama1b_steps5_rank_heatmap_page5.png
  Saved: results_ane/llama1b_steps5_rank_heatmap_page6.png
  Saved: results_ane/llama1b_steps5_subspace_evolution.png
  Saved: results_ane/llama1b_steps5_top10_evolution.png
  Saved: results_ane/llama1b_steps5_flipped_cases.png
  Saved: results_ane/llama1b_steps5_summary.png
All visualizations saved to results_ane/

>>> 1B Model, 30 steps
2026-01-13 00:26:12.525792: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-13 00:26:12.580976: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

======================================================================
ANE-TTA v3 Experiment: llama1b_steps30
======================================================================
Model: Llama-3.2-1B-Instruct
Model size: 1.2B parameters
Numerical tokens: 1110/128000
Model size: 1.2B params
Training 16/16 layers
Update target: attn+ln
Trainable params: 167.8M (13.6%)
Processing samples:   0%|          | 0/50 [00:00<?, ?it/s]Processing samples:   2%|▏         | 1/50 [00:02<01:52,  2.30s/it]Processing samples:   4%|▍         | 2/50 [00:04<01:36,  2.02s/it]Processing samples:   6%|▌         | 3/50 [00:07<02:09,  2.76s/it]Processing samples:   8%|▊         | 4/50 [00:11<02:22,  3.10s/it]Processing samples:  10%|█         | 5/50 [00:15<02:28,  3.30s/it]Processing samples:  12%|█▏        | 6/50 [00:20<02:56,  4.02s/it]Processing samples:  14%|█▍        | 7/50 [00:25<03:13,  4.51s/it]Processing samples:  16%|█▌        | 8/50 [00:31<03:23,  4.84s/it]Processing samples:  18%|█▊        | 9/50 [00:38<03:50,  5.63s/it]Processing samples:  20%|██        | 10/50 [00:46<04:10,  6.26s/it]Processing samples:  22%|██▏       | 11/50 [00:53<04:16,  6.56s/it]Processing samples:  24%|██▍       | 12/50 [01:01<04:22,  6.90s/it]Processing samples:  26%|██▌       | 13/50 [01:10<04:44,  7.69s/it]Processing samples:  28%|██▊       | 14/50 [01:20<04:56,  8.23s/it]Processing samples:  30%|███       | 15/50 [01:29<05:01,  8.60s/it]Processing samples:  32%|███▏      | 16/50 [01:41<05:17,  9.35s/it]Processing samples:  34%|███▍      | 17/50 [01:52<05:29,  9.98s/it]Processing samples:  36%|███▌      | 18/50 [02:05<05:51, 10.98s/it]Processing samples:  38%|███▊      | 19/50 [02:19<06:01, 11.67s/it]Processing samples:  40%|████      | 20/50 [02:32<06:03, 12.13s/it]Processing samples:  42%|████▏     | 21/50 [02:47<06:16, 12.97s/it]Processing samples:  44%|████▍     | 22/50 [03:02<06:22, 13.65s/it]Processing samples:  46%|████▌     | 23/50 [03:16<06:14, 13.88s/it]Processing samples:  48%|████▊     | 24/50 [03:33<06:25, 14.84s/it]Processing samples:  50%|█████     | 25/50 [03:50<06:27, 15.51s/it]Processing samples:  52%|█████▏    | 26/50 [04:07<06:17, 15.74s/it]Processing samples:  54%|█████▍    | 27/50 [04:26<06:25, 16.75s/it]Processing samples:  56%|█████▌    | 28/50 [04:44<06:20, 17.32s/it]Processing samples:  58%|█████▊    | 29/50 [05:03<06:09, 17.61s/it]Processing samples:  60%|██████    | 30/50 [05:22<06:01, 18.06s/it]Processing samples:  62%|██████▏   | 31/50 [05:43<05:58, 18.87s/it]Processing samples:  64%|██████▍   | 32/50 [06:03<05:50, 19.45s/it]Processing samples:  66%|██████▌   | 33/50 [06:24<05:36, 19.81s/it]Processing samples:  68%|██████▊   | 34/50 [06:46<05:29, 20.57s/it]Processing samples:  70%|███████   | 35/50 [07:08<05:12, 20.86s/it]Processing samples:  72%|███████▏  | 36/50 [07:32<05:06, 21.92s/it]Processing samples:  74%|███████▍  | 37/50 [07:56<04:53, 22.56s/it]Processing samples:  76%|███████▌  | 38/50 [08:21<04:38, 23.20s/it]Processing samples:  78%|███████▊  | 39/50 [08:47<04:25, 24.13s/it]Processing samples:  80%|████████  | 40/50 [09:14<04:08, 24.87s/it]Processing samples:  82%|████████▏ | 41/50 [09:40<03:46, 25.19s/it]Processing samples:  84%|████████▍ | 42/50 [10:09<03:29, 26.22s/it]Processing samples:  86%|████████▌ | 43/50 [10:37<03:08, 26.94s/it]Processing samples:  88%|████████▊ | 44/50 [11:06<02:44, 27.43s/it]Processing samples:  90%|█████████ | 45/50 [11:35<02:20, 28.06s/it]Processing samples:  92%|█████████▏| 46/50 [12:05<01:54, 28.58s/it]Processing samples:  94%|█████████▍| 47/50 [12:35<01:27, 29.05s/it]Processing samples:  96%|█████████▌| 48/50 [13:05<00:58, 29.32s/it]Processing samples:  98%|█████████▊| 49/50 [13:37<00:30, 30.03s/it]Processing samples: 100%|██████████| 50/50 [14:09<00:00, 30.63s/it]Processing samples: 100%|██████████| 50/50 [14:09<00:00, 16.99s/it]

======================================================================
Results Summary
======================================================================
Total tokens: 453
Accuracy Before TTA: 0.0287 (13/453)
Accuracy After TTA:  0.0442 (20/453)
Improvement: +0.0155
Flipped cases (wrong->correct): 8
Total time: 14.3 min
Time per token: 1.89 sec
======================================================================
Results saved to: results_ane/llama1b_steps30.json

Generating visualizations...
Generating visualizations for llama1b_steps30...
  Saved: results_ane/llama1b_steps30_prob_heatmap_page1.png
  Saved: results_ane/llama1b_steps30_prob_heatmap_page2.png
  Saved: results_ane/llama1b_steps30_prob_heatmap_page3.png
  Saved: results_ane/llama1b_steps30_prob_heatmap_page4.png
  Saved: results_ane/llama1b_steps30_prob_heatmap_page5.png
  Saved: results_ane/llama1b_steps30_prob_heatmap_page6.png
  Saved: results_ane/llama1b_steps30_rank_heatmap_page1.png
  Saved: results_ane/llama1b_steps30_rank_heatmap_page2.png
  Saved: results_ane/llama1b_steps30_rank_heatmap_page3.png
  Saved: results_ane/llama1b_steps30_rank_heatmap_page4.png
  Saved: results_ane/llama1b_steps30_rank_heatmap_page5.png
  Saved: results_ane/llama1b_steps30_rank_heatmap_page6.png
  Saved: results_ane/llama1b_steps30_subspace_evolution.png
  Saved: results_ane/llama1b_steps30_top10_evolution.png
  Saved: results_ane/llama1b_steps30_flipped_cases.png
  Saved: results_ane/llama1b_steps30_summary.png
All visualizations saved to results_ane/

=== Llama-3.2-3B Experiments ===

>>> 3B Model, 5 steps
2026-01-13 00:40:49.165738: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-13 00:40:49.221095: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

======================================================================
ANE-TTA v3 Experiment: llama3b_steps5
======================================================================
Model: Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.92s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.24s/it]
Model size: 3.2B parameters
Numerical tokens: 1110/128000
Model size: 3.2B params
Training 28/28 layers
Update target: attn+ln
Trainable params: 704.8M (21.9%)
Processing samples:   0%|          | 0/50 [00:00<?, ?it/s]Processing samples:   2%|▏         | 1/50 [00:01<00:53,  1.09s/it]Processing samples:   4%|▍         | 2/50 [00:01<00:36,  1.30it/s]Processing samples:   6%|▌         | 3/50 [00:02<00:43,  1.09it/s]Processing samples:   8%|▊         | 4/50 [00:03<00:45,  1.01it/s]Processing samples:  10%|█         | 5/50 [00:04<00:46,  1.04s/it]Processing samples:  12%|█▏        | 6/50 [00:06<00:54,  1.25s/it]Processing samples:  14%|█▍        | 7/50 [00:08<00:59,  1.38s/it]Processing samples:  16%|█▌        | 8/50 [00:09<01:00,  1.44s/it]Processing samples:  18%|█▊        | 9/50 [00:12<01:08,  1.68s/it]Processing samples:  20%|██        | 10/50 [00:14<01:13,  1.85s/it]Processing samples:  22%|██▏       | 11/50 [00:16<01:16,  1.96s/it]Processing samples:  24%|██▍       | 12/50 [00:18<01:17,  2.04s/it]Processing samples:  26%|██▌       | 13/50 [00:21<01:21,  2.20s/it]Processing samples:  28%|██▊       | 14/50 [00:23<01:23,  2.32s/it]Processing samples:  30%|███       | 15/50 [00:26<01:25,  2.45s/it]Processing samples:  32%|███▏      | 16/50 [00:29<01:32,  2.71s/it]Processing samples:  34%|███▍      | 17/50 [00:33<01:34,  2.87s/it]Processing samples:  36%|███▌      | 18/50 [00:37<01:42,  3.19s/it]Processing samples:  38%|███▊      | 19/50 [00:41<01:46,  3.43s/it]Processing samples:  40%|████      | 20/50 [00:45<01:47,  3.58s/it]Processing samples:  42%|████▏     | 21/50 [00:49<01:50,  3.82s/it]Processing samples:  44%|████▍     | 22/50 [00:53<01:53,  4.05s/it]Processing samples:  46%|████▌     | 23/50 [00:58<01:53,  4.20s/it]Processing samples:  48%|████▊     | 24/50 [01:03<01:55,  4.44s/it]Processing samples:  50%|█████     | 25/50 [01:08<01:55,  4.60s/it]Processing samples:  52%|█████▏    | 26/50 [01:13<01:52,  4.67s/it]Processing samples:  54%|█████▍    | 27/50 [01:18<01:52,  4.91s/it]Processing samples:  56%|█████▌    | 28/50 [01:24<01:52,  5.09s/it]Processing samples:  58%|█████▊    | 29/50 [01:29<01:49,  5.22s/it]Processing samples:  60%|██████    | 30/50 [01:35<01:46,  5.30s/it]Processing samples:  62%|██████▏   | 31/50 [01:41<01:45,  5.53s/it]Processing samples:  64%|██████▍   | 32/50 [01:47<01:42,  5.68s/it]Processing samples:  66%|██████▌   | 33/50 [01:53<01:38,  5.77s/it]Processing samples:  68%|██████▊   | 34/50 [01:59<01:35,  5.97s/it]Processing samples:  70%|███████   | 35/50 [02:06<01:31,  6.12s/it]Processing samples:  72%|███████▏  | 36/50 [02:13<01:30,  6.47s/it]Processing samples:  74%|███████▍  | 37/50 [02:21<01:27,  6.77s/it]Processing samples:  76%|███████▌  | 38/50 [02:28<01:23,  6.99s/it]Processing samples:  78%|███████▊  | 39/50 [02:36<01:19,  7.25s/it]Processing samples:  80%|████████  | 40/50 [02:43<01:13,  7.34s/it]Processing samples:  82%|████████▏ | 41/50 [02:51<01:06,  7.38s/it]Processing samples:  84%|████████▍ | 42/50 [02:59<01:00,  7.61s/it]Processing samples:  86%|████████▌ | 43/50 [03:07<00:54,  7.79s/it]Processing samples:  88%|████████▊ | 44/50 [03:16<00:47,  7.95s/it]Processing samples:  90%|█████████ | 45/50 [03:24<00:40,  8.14s/it]Processing samples:  92%|█████████▏| 46/50 [03:33<00:33,  8.35s/it]Processing samples:  94%|█████████▍| 47/50 [03:42<00:25,  8.48s/it]Processing samples:  96%|█████████▌| 48/50 [03:51<00:17,  8.56s/it]Processing samples:  98%|█████████▊| 49/50 [04:00<00:08,  8.78s/it]Processing samples: 100%|██████████| 50/50 [04:09<00:00,  8.94s/it]Processing samples: 100%|██████████| 50/50 [04:09<00:00,  4.99s/it]

======================================================================
Results Summary
======================================================================
Total tokens: 453
Accuracy Before TTA: 0.1170 (53/453)
Accuracy After TTA:  0.1236 (56/453)
Improvement: +0.0066
Flipped cases (wrong->correct): 5
Total time: 4.3 min
Time per token: 0.57 sec
======================================================================
Results saved to: results_ane/llama3b_steps5.json

Generating visualizations...
Generating visualizations for llama3b_steps5...
  Saved: results_ane/llama3b_steps5_prob_heatmap_page1.png
  Saved: results_ane/llama3b_steps5_prob_heatmap_page2.png
  Saved: results_ane/llama3b_steps5_prob_heatmap_page3.png
  Saved: results_ane/llama3b_steps5_prob_heatmap_page4.png
  Saved: results_ane/llama3b_steps5_prob_heatmap_page5.png
  Saved: results_ane/llama3b_steps5_prob_heatmap_page6.png
  Saved: results_ane/llama3b_steps5_rank_heatmap_page1.png
  Saved: results_ane/llama3b_steps5_rank_heatmap_page2.png
  Saved: results_ane/llama3b_steps5_rank_heatmap_page3.png
  Saved: results_ane/llama3b_steps5_rank_heatmap_page4.png
  Saved: results_ane/llama3b_steps5_rank_heatmap_page5.png
  Saved: results_ane/llama3b_steps5_rank_heatmap_page6.png
  Saved: results_ane/llama3b_steps5_subspace_evolution.png
  Saved: results_ane/llama3b_steps5_top10_evolution.png
  Saved: results_ane/llama3b_steps5_flipped_cases.png
  Saved: results_ane/llama3b_steps5_summary.png
All visualizations saved to results_ane/

>>> 3B Model, 30 steps
2026-01-13 00:45:24.997403: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-13 00:45:25.052287: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

======================================================================
ANE-TTA v3 Experiment: llama3b_steps30
======================================================================
Model: Llama-3.2-3B-Instruct
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.03s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.14s/it]
Model size: 3.2B parameters
Numerical tokens: 1110/128000
Model size: 3.2B params
Training 28/28 layers
Update target: attn+ln
Trainable params: 704.8M (21.9%)
Processing samples:   0%|          | 0/50 [00:00<?, ?it/s]Processing samples:   2%|▏         | 1/50 [00:03<02:39,  3.26s/it]Processing samples:   4%|▍         | 2/50 [00:06<02:22,  2.96s/it]Processing samples:   6%|▌         | 3/50 [00:11<03:13,  4.12s/it]Processing samples:   8%|▊         | 4/50 [00:17<03:34,  4.66s/it]Processing samples:  10%|█         | 5/50 [00:22<03:43,  4.97s/it]Processing samples:  12%|█▏        | 6/50 [00:30<04:27,  6.08s/it]Processing samples:  14%|█▍        | 7/50 [00:38<04:45,  6.63s/it]Processing samples:  16%|█▌        | 8/50 [00:46<04:56,  7.06s/it]Processing samples:  18%|█▊        | 9/50 [00:57<05:34,  8.17s/it]Processing samples:  20%|██        | 10/50 [01:08<06:01,  9.04s/it]Processing samples:  22%|██▏       | 11/50 [01:19<06:15,  9.64s/it]Processing samples:  24%|██▍       | 12/50 [01:30<06:21, 10.05s/it]Processing samples:  26%|██▌       | 13/50 [01:43<06:50, 11.08s/it]Processing samples:  28%|██▊       | 14/50 [01:56<07:03, 11.78s/it]Processing samples:  30%|███       | 15/50 [02:10<07:08, 12.24s/it]Processing samples:  32%|███▏      | 16/50 [02:26<07:39, 13.51s/it]Processing samples:  34%|███▍      | 17/50 [02:43<07:54, 14.39s/it]Processing samples:  36%|███▌      | 18/50 [03:02<08:26, 15.82s/it]Processing samples:  38%|███▊      | 19/50 [03:21<08:40, 16.80s/it]Processing samples:  40%|████      | 20/50 [03:40<08:45, 17.51s/it]Processing samples:  42%|████▏     | 21/50 [04:01<08:59, 18.60s/it]Processing samples:  44%|████▍     | 22/50 [04:22<09:01, 19.34s/it]Processing samples:  46%|████▌     | 23/50 [04:44<09:03, 20.12s/it]Processing samples:  48%|████▊     | 24/50 [05:08<09:13, 21.29s/it]Processing samples:  50%|█████     | 25/50 [05:33<09:17, 22.30s/it]Processing samples:  52%|█████▏    | 26/50 [05:57<09:12, 23.00s/it]Processing samples:  54%|█████▍    | 27/50 [06:25<09:19, 24.31s/it]Processing samples:  56%|█████▌    | 28/50 [06:52<09:12, 25.12s/it]Processing samples:  58%|█████▊    | 29/50 [07:19<08:58, 25.64s/it]Processing samples:  60%|██████    | 30/50 [07:46<08:42, 26.14s/it]Processing samples:  62%|██████▏   | 31/50 [08:16<08:36, 27.18s/it]Processing samples:  64%|██████▍   | 32/50 [08:46<08:23, 27.99s/it]Processing samples:  66%|██████▌   | 33/50 [09:16<08:06, 28.64s/it]Processing samples:  68%|██████▊   | 34/50 [09:48<07:58, 29.88s/it]Processing samples:  70%|███████   | 35/50 [10:21<07:39, 30.61s/it]Processing samples:  72%|███████▏  | 36/50 [10:56<07:28, 32.00s/it]Processing samples:  74%|███████▍  | 37/50 [11:32<07:10, 33.11s/it]Processing samples:  76%|███████▌  | 38/50 [12:07<06:45, 33.79s/it]Processing samples:  78%|███████▊  | 39/50 [12:45<06:25, 35.05s/it]Processing samples:  80%|████████  | 40/50 [13:24<06:00, 36.09s/it]Processing samples:  82%|████████▏ | 41/50 [14:02<05:31, 36.80s/it]Processing samples:  84%|████████▍ | 42/50 [14:43<05:04, 38.01s/it]Processing samples:  86%|████████▌ | 43/50 [15:24<04:32, 38.92s/it]Processing samples:  88%|████████▊ | 44/50 [16:05<03:56, 39.50s/it]Processing samples:  90%|█████████ | 45/50 [16:48<03:23, 40.68s/it]Processing samples:  92%|█████████▏| 46/50 [17:32<02:46, 41.61s/it]Processing samples:  94%|█████████▍| 47/50 [18:15<02:06, 42.18s/it]Processing samples:  96%|█████████▌| 48/50 [18:59<01:24, 42.45s/it]Processing samples:  98%|█████████▊| 49/50 [19:45<00:43, 43.55s/it]Processing samples: 100%|██████████| 50/50 [20:30<00:00, 44.11s/it]Processing samples: 100%|██████████| 50/50 [20:30<00:00, 24.61s/it]

======================================================================
Results Summary
======================================================================
Total tokens: 453
Accuracy Before TTA: 0.1170 (53/453)
Accuracy After TTA:  0.2053 (93/453)
Improvement: +0.0883
Flipped cases (wrong->correct): 43
Total time: 20.6 min
Time per token: 2.73 sec
======================================================================
Results saved to: results_ane/llama3b_steps30.json

Generating visualizations...
Generating visualizations for llama3b_steps30...
  Saved: results_ane/llama3b_steps30_prob_heatmap_page1.png
  Saved: results_ane/llama3b_steps30_prob_heatmap_page2.png
  Saved: results_ane/llama3b_steps30_prob_heatmap_page3.png
  Saved: results_ane/llama3b_steps30_prob_heatmap_page4.png
  Saved: results_ane/llama3b_steps30_prob_heatmap_page5.png
  Saved: results_ane/llama3b_steps30_prob_heatmap_page6.png
  Saved: results_ane/llama3b_steps30_rank_heatmap_page1.png
  Saved: results_ane/llama3b_steps30_rank_heatmap_page2.png
  Saved: results_ane/llama3b_steps30_rank_heatmap_page3.png
  Saved: results_ane/llama3b_steps30_rank_heatmap_page4.png
  Saved: results_ane/llama3b_steps30_rank_heatmap_page5.png
  Saved: results_ane/llama3b_steps30_rank_heatmap_page6.png
  Saved: results_ane/llama3b_steps30_subspace_evolution.png
  Saved: results_ane/llama3b_steps30_top10_evolution.png
  Saved: results_ane/llama3b_steps30_flipped_cases.png
  Saved: results_ane/llama3b_steps30_summary.png
All visualizations saved to results_ane/

=== Llama-3.1-8B Experiments (16 layers for memory efficiency) ===

>>> 8B Model, 5 steps (16 layers)
2026-01-13 01:06:23.445647: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-13 01:06:23.500942: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

======================================================================
ANE-TTA v3 Experiment: llama8b_steps5
======================================================================
Model: Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.77s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.75s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]
Model size: 8.0B parameters
Numerical tokens: 1110/128000
Model size: 8.0B params
Training 16/32 layers
Update target: attn+ln
Trainable params: 671.2M (8.4%)
Processing samples:   0%|          | 0/50 [00:00<?, ?it/s]Processing samples:   2%|▏         | 1/50 [00:01<00:49,  1.01s/it]Processing samples:   4%|▍         | 2/50 [00:01<00:33,  1.42it/s]Processing samples:   6%|▌         | 3/50 [00:02<00:40,  1.17it/s]Processing samples:   8%|▊         | 4/50 [00:03<00:42,  1.08it/s]Processing samples:  10%|█         | 5/50 [00:04<00:43,  1.03it/s]Processing samples:  12%|█▏        | 6/50 [00:06<00:50,  1.16s/it]Processing samples:  14%|█▍        | 7/50 [00:07<00:55,  1.28s/it]Processing samples:  16%|█▌        | 8/50 [00:09<00:57,  1.36s/it]Processing samples:  18%|█▊        | 9/50 [00:11<01:04,  1.58s/it]Processing samples:  20%|██        | 10/50 [00:13<01:09,  1.73s/it]Processing samples:  22%|██▏       | 11/50 [00:15<01:11,  1.83s/it]Processing samples:  24%|██▍       | 12/50 [00:17<01:12,  1.91s/it]Processing samples:  26%|██▌       | 13/50 [00:20<01:17,  2.11s/it]Processing samples:  28%|██▊       | 14/50 [00:22<01:20,  2.25s/it]Processing samples:  30%|███       | 15/50 [00:25<01:22,  2.34s/it]Processing samples:  32%|███▏      | 16/50 [00:28<01:27,  2.56s/it]Processing samples:  34%|███▍      | 17/50 [00:31<01:29,  2.72s/it]Processing samples:  36%|███▌      | 18/50 [00:34<01:35,  2.98s/it]Processing samples:  38%|███▊      | 19/50 [00:38<01:38,  3.17s/it]Processing samples:  40%|████      | 20/50 [00:42<01:38,  3.29s/it]Processing samples:  42%|████▏     | 21/50 [00:46<01:42,  3.54s/it]Processing samples:  44%|████▍     | 22/50 [00:50<01:43,  3.71s/it]Processing samples:  46%|████▌     | 23/50 [00:54<01:43,  3.83s/it]Processing samples:  48%|████▊     | 24/50 [00:59<01:45,  4.06s/it]Processing samples:  50%|█████     | 25/50 [01:03<01:45,  4.23s/it]Processing samples:  52%|█████▏    | 26/50 [01:08<01:44,  4.34s/it]Processing samples:  54%|█████▍    | 27/50 [01:13<01:45,  4.58s/it]Processing samples:  56%|█████▌    | 28/50 [01:18<01:44,  4.75s/it]Processing samples:  58%|█████▊    | 29/50 [01:23<01:42,  4.86s/it]Processing samples:  60%|██████    | 30/50 [01:28<01:38,  4.95s/it]Processing samples:  62%|██████▏   | 31/50 [01:34<01:37,  5.13s/it]Processing samples:  64%|██████▍   | 32/50 [01:39<01:34,  5.27s/it]Processing samples:  66%|██████▌   | 33/50 [01:45<01:31,  5.37s/it]Processing samples:  68%|██████▊   | 34/50 [01:51<01:29,  5.60s/it]Processing samples:  70%|███████   | 35/50 [01:57<01:26,  5.76s/it]Processing samples:  72%|███████▏  | 36/50 [02:04<01:24,  6.03s/it]Processing samples:  74%|███████▍  | 37/50 [02:11<01:20,  6.21s/it]Processing samples:  76%|███████▌  | 38/50 [02:17<01:16,  6.34s/it]Processing samples:  78%|███████▊  | 39/50 [02:24<01:12,  6.57s/it]Processing samples:  80%|████████  | 40/50 [02:32<01:07,  6.75s/it]Processing samples:  82%|████████▏ | 41/50 [02:39<01:01,  6.87s/it]Processing samples:  84%|████████▍ | 42/50 [02:46<00:56,  7.10s/it]Processing samples:  86%|████████▌ | 43/50 [02:54<00:50,  7.27s/it]Processing samples:  88%|████████▊ | 44/50 [03:02<00:44,  7.39s/it]Processing samples:  90%|█████████ | 45/50 [03:10<00:38,  7.62s/it]Processing samples:  92%|█████████▏| 46/50 [03:18<00:31,  7.80s/it]Processing samples:  94%|█████████▍| 47/50 [03:25<00:22,  7.63s/it]Processing samples:  96%|█████████▌| 48/50 [03:34<00:15,  7.83s/it]Processing samples:  98%|█████████▊| 49/50 [03:43<00:08,  8.17s/it]Processing samples: 100%|██████████| 50/50 [03:51<00:00,  8.39s/it]Processing samples: 100%|██████████| 50/50 [03:51<00:00,  4.64s/it]

======================================================================
Results Summary
======================================================================
Total tokens: 453
Accuracy Before TTA: 0.2737 (124/453)
Accuracy After TTA:  0.2826 (128/453)
Improvement: +0.0088
Flipped cases (wrong->correct): 4
Total time: 4.0 min
Time per token: 0.54 sec
======================================================================
Results saved to: results_ane/llama8b_steps5.json

Generating visualizations...
Generating visualizations for llama8b_steps5...
  Saved: results_ane/llama8b_steps5_prob_heatmap_page1.png
  Saved: results_ane/llama8b_steps5_prob_heatmap_page2.png
  Saved: results_ane/llama8b_steps5_prob_heatmap_page3.png
  Saved: results_ane/llama8b_steps5_prob_heatmap_page4.png
  Saved: results_ane/llama8b_steps5_prob_heatmap_page5.png
  Saved: results_ane/llama8b_steps5_prob_heatmap_page6.png
  Saved: results_ane/llama8b_steps5_rank_heatmap_page1.png
  Saved: results_ane/llama8b_steps5_rank_heatmap_page2.png
  Saved: results_ane/llama8b_steps5_rank_heatmap_page3.png
  Saved: results_ane/llama8b_steps5_rank_heatmap_page4.png
  Saved: results_ane/llama8b_steps5_rank_heatmap_page5.png
  Saved: results_ane/llama8b_steps5_rank_heatmap_page6.png
  Saved: results_ane/llama8b_steps5_subspace_evolution.png
  Saved: results_ane/llama8b_steps5_top10_evolution.png
  Saved: results_ane/llama8b_steps5_flipped_cases.png
  Saved: results_ane/llama8b_steps5_summary.png
All visualizations saved to results_ane/

>>> 8B Model, 30 steps (16 layers)
2026-01-13 01:10:46.461608: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2026-01-13 01:10:46.516481: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

======================================================================
ANE-TTA v3 Experiment: llama8b_steps30
======================================================================
Model: Llama-3.1-8B-Instruct
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:05,  1.74s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:03<00:03,  1.76s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:05<00:01,  1.74s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.41s/it]
Model size: 8.0B parameters
Numerical tokens: 1110/128000
Model size: 8.0B params
Training 16/32 layers
Update target: attn+ln
Trainable params: 671.2M (8.4%)
Processing samples:   0%|          | 0/50 [00:00<?, ?it/s]Processing samples:   2%|▏         | 1/50 [00:03<02:34,  3.16s/it]Processing samples:   4%|▍         | 2/50 [00:05<02:18,  2.89s/it]Processing samples:   6%|▌         | 3/50 [00:11<03:10,  4.05s/it]Processing samples:   8%|▊         | 4/50 [00:16<03:20,  4.35s/it]Processing samples:  10%|█         | 5/50 [00:21<03:33,  4.74s/it]Processing samples:  12%|█▏        | 6/50 [00:29<04:17,  5.86s/it]Processing samples:  14%|█▍        | 7/50 [00:37<04:36,  6.42s/it]Processing samples:  16%|█▌        | 8/50 [00:45<04:52,  6.97s/it]Processing samples:  18%|█▊        | 9/50 [00:56<05:35,  8.18s/it]Processing samples:  20%|██        | 10/50 [01:07<06:00,  9.01s/it]Processing samples:  22%|██▏       | 11/50 [01:17<06:13,  9.58s/it]Processing samples:  24%|██▍       | 12/50 [01:28<06:18,  9.97s/it]Processing samples:  26%|██▌       | 13/50 [01:42<06:49, 11.07s/it]Processing samples:  28%|██▊       | 14/50 [01:55<07:05, 11.83s/it]Processing samples:  30%|███       | 15/50 [02:09<07:12, 12.36s/it]Processing samples:  32%|███▏      | 16/50 [02:25<07:40, 13.54s/it]Processing samples:  34%|███▍      | 17/50 [02:42<07:54, 14.37s/it]Processing samples:  36%|███▌      | 18/50 [03:01<08:24, 15.77s/it]Processing samples:  38%|███▊      | 19/50 [03:19<08:35, 16.63s/it]Processing samples:  40%|████      | 20/50 [03:38<08:39, 17.32s/it]Processing samples:  42%|████▏     | 21/50 [04:00<09:00, 18.65s/it]Processing samples:  44%|████▍     | 22/50 [04:22<09:08, 19.58s/it]Processing samples:  46%|████▌     | 23/50 [04:43<09:01, 20.05s/it]Processing samples:  48%|████▊     | 24/50 [05:07<09:15, 21.38s/it]Processing samples:  50%|█████     | 25/50 [05:31<09:09, 21.98s/it]Processing samples:  52%|█████▏    | 26/50 [05:55<09:04, 22.68s/it]Processing samples:  54%|█████▍    | 27/50 [06:20<09:00, 23.51s/it]Processing samples:  56%|█████▌    | 28/50 [06:48<09:01, 24.61s/it]Processing samples:  58%|█████▊    | 29/50 [07:15<08:53, 25.39s/it]Processing samples:  60%|██████    | 30/50 [07:42<08:38, 25.95s/it]Processing samples:  62%|██████▏   | 31/50 [08:12<08:36, 27.16s/it]Processing samples:  64%|██████▍   | 32/50 [08:39<08:06, 27.04s/it]Processing samples:  66%|██████▌   | 33/50 [09:08<07:51, 27.71s/it]Processing samples:  68%|██████▊   | 34/50 [09:41<07:47, 29.25s/it]Processing samples:  70%|███████   | 35/50 [10:11<07:23, 29.54s/it]Processing samples:  72%|███████▏  | 36/50 [10:44<07:08, 30.58s/it]Processing samples:  74%|███████▍  | 37/50 [11:18<06:50, 31.59s/it]Processing samples:  76%|███████▌  | 38/50 [11:54<06:33, 32.79s/it]Processing samples:  78%|███████▊  | 39/50 [12:31<06:15, 34.12s/it]Processing samples:  80%|████████  | 40/50 [13:07<05:48, 34.84s/it]Processing samples:  82%|████████▏ | 41/50 [13:39<05:04, 33.83s/it]Processing samples:  84%|████████▍ | 42/50 [14:13<04:30, 33.82s/it]Processing samples:  86%|████████▌ | 43/50 [14:49<04:01, 34.47s/it]Processing samples:  88%|████████▊ | 44/50 [15:27<03:32, 35.47s/it]Processing samples:  90%|█████████ | 45/50 [16:07<03:05, 37.01s/it]Processing samples:  92%|█████████▏| 46/50 [16:51<02:36, 39.09s/it]Processing samples:  94%|█████████▍| 47/50 [17:35<02:01, 40.52s/it]Processing samples:  96%|█████████▌| 48/50 [18:12<01:18, 39.35s/it]Processing samples:  98%|█████████▊| 49/50 [18:50<00:39, 39.22s/it]Processing samples: 100%|██████████| 50/50 [19:32<00:00, 39.83s/it]Processing samples: 100%|██████████| 50/50 [19:32<00:00, 23.44s/it]

======================================================================
Results Summary
======================================================================
Total tokens: 453
Accuracy Before TTA: 0.2737 (124/453)
Accuracy After TTA:  0.3024 (137/453)
Improvement: +0.0287
Flipped cases (wrong->correct): 15
Total time: 19.7 min
Time per token: 2.61 sec
======================================================================
Results saved to: results_ane/llama8b_steps30.json

Generating visualizations...
Generating visualizations for llama8b_steps30...
  Saved: results_ane/llama8b_steps30_prob_heatmap_page1.png
  Saved: results_ane/llama8b_steps30_prob_heatmap_page2.png
  Saved: results_ane/llama8b_steps30_prob_heatmap_page3.png
  Saved: results_ane/llama8b_steps30_prob_heatmap_page4.png
  Saved: results_ane/llama8b_steps30_prob_heatmap_page5.png
  Saved: results_ane/llama8b_steps30_prob_heatmap_page6.png
  Saved: results_ane/llama8b_steps30_rank_heatmap_page1.png
  Saved: results_ane/llama8b_steps30_rank_heatmap_page2.png
  Saved: results_ane/llama8b_steps30_rank_heatmap_page3.png
  Saved: results_ane/llama8b_steps30_rank_heatmap_page4.png
  Saved: results_ane/llama8b_steps30_rank_heatmap_page5.png
  Saved: results_ane/llama8b_steps30_rank_heatmap_page6.png
  Saved: results_ane/llama8b_steps30_subspace_evolution.png
  Saved: results_ane/llama8b_steps30_top10_evolution.png
  Saved: results_ane/llama8b_steps30_flipped_cases.png
  Saved: results_ane/llama8b_steps30_summary.png
All visualizations saved to results_ane/

==========================================
All Production Experiments Complete!
==========================================
Results saved to: results_ane/
